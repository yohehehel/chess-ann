# Chess Game Outcome Prediction - Project Architecture

## Overview

This project is an end-to-end pipeline for predicting the outcome of a chess game (win, loss, or draw) after a fixed number of moves using an Artificial Neural Network (ANN). The model is trained on a Lichess games dataset and attempts to estimate the probability of a White win, Black win, or draw given the state of the game after _n_ moves. The implementation is done in Python using a single Jupyter Notebook for ease of experimentation and visualization. Key components of the project include:

- **Data Ingestion & Preprocessing:** Reading the chess games dataset and preparing input features (derived from moves and game state) and output labels (game results).
- **Model Definition:** Designing a neural network architecture to take a chess position as input and output a prediction of the game result.
- **Model Training:** Training the ANN on historical game data, with proper train/test splits and monitoring of performance metrics.
- **Evaluation:** Assessing the model's accuracy on test data and analyzing its performance (including handling three outcome classes if possible).
- **User Interface & Visualization:** Presenting training progress and evaluation results in a graphical manner - including charts of training metrics and visual chess board snapshots with predicted vs actual outcomes.

By combining these elements, the project delivers a functional prototype that not only trains an outcome-prediction model but also visually demonstrates the model's predictions on example game positions.

## Project Structure

The project is organized with a clear file and directory structure to separate data, code, models, and visuals. Below is an outline of the repository structure and the role of each component:
```
chess-outcome-prediction/  
├── data/  
│ └── lichess_games.csv # Dataset of chess games (Lichess format with moves and metadata)  
├── ChessOutcomePrediction.ipynb # Jupyter Notebook containing the entire code pipeline  
├── models/  
│ └── best_model.h5 # Saved trained model (weights/architecture) for reuse (optional)  
├── visuals/  
│ ├── training_curves.png # Plot of training/validation accuracy & loss (generated during training)  
│ └── example_position.svg # Example chess board image for a test position (for visualization)  
├── ui/  
│ └── index.html # (Optional) Local HTML page for an interactive UI to display results  
└── architecture.md # Architecture documentation (this file)
```
- **data/lichess_games.csv:** This is the dataset containing the chess games used for training and testing. Each record includes information about a game: players, their ratings, game metadata, the moves, and the final result. Notably, it contains fields such as **WhiteElo**, **BlackElo** (players' ratings), **WhiteRatingDiff**/**BlackRatingDiff** (rating change after the game), **TimeControl**, **Opening**, **Termination**, and **AN** (Algebraic Notation moves) among others[\[1\]](https://huggingface.co/api/resolve-cache/datasets/Lichess/standard-chess-games/a9be553ea50eeaf58129778d7875ceaca241d935/README.md?download=true&etag=%227486643c201bc6569cf0176d540d01c426410465%22#:~:text=features%3A%20,lichess). The moves are recorded in standard algebraic notation (movetext PGN format) and the result is given as "1-0", "0-1", or "1/2-1/2" (denoting White win, Black win, or draw respectively).
- **ChessOutcomePrediction.ipynb:** This is the core Jupyter Notebook that holds the entire code. It is structured into sequential sections (cells) for each stage of the pipeline: data loading, preprocessing, model building, training, evaluation, and visualization. Using a single notebook makes it easier to run the whole project step by step and see outputs (graphs, predictions) inline. The notebook contains defined helper functions (for example, to parse moves or display boards) and uses markdown headings to separate logical sections. We detail the contents and flow of this notebook in the next sections.
- **models/best_model.h5:** After training, the best-performing model (in terms of validation accuracy) can be saved here. This file stores the model architecture and weights so that the trained ANN can be re-loaded later without retraining. (This is optional; the notebook will also allow running predictions immediately after training without needing to save, but saving is good practice for persistence.)
- **visuals/:** This folder stores generated visuals that are used in the user interface or analysis. For example, **training_curves.png** is a plot showing how the model's accuracy and loss progressed over training epochs, and **example_position.svg** could be an SVG image of a chess board from a test game used for demonstrating a prediction. These files are created by the notebook during execution. Storing them ensures they can be embedded in the UI or reports.
- **ui/index.html:** This is an optional lightweight web page that can be opened locally (e.g., via a browser on localhost) to present the results in a user-friendly way. It might display the training charts and some example predictions with chess board graphics. The content for this page is generated by the notebook - for instance, the notebook can export charts or use JavaScript libraries to embed chess boards. If a separate UI page is not used, the notebook itself serves as the interface (since Jupyter can display graphs and images inline).
- **architecture.md:** Documentation of the project architecture (the file you are reading). It explains the design decisions, file organization, and how different parts of the code interact. This is meant to help developers or stakeholders understand the project's structure and logic without digging directly into code.

By organizing the project in this manner, each component has a clear responsibility, and the connections between components are well-defined. Next, we walk through the pipeline as implemented in the ChessOutcomePrediction.ipynb notebook, detailing each part of the workflow and how functions and data flow between them.

## Data Ingestion & Preprocessing

**Loading the Dataset:** In the first section of the notebook, the dataset CSV file is loaded using pandas. We read all game records into a DataFrame, which gives us access to columns like _Event_, _White_, _Black_, _WhiteElo_, _BlackElo_, _Result_, _Termination_, _TimeControl_, and the moves (_AN_ or movetext) among others[\[1\]](https://huggingface.co/api/resolve-cache/datasets/Lichess/standard-chess-games/a9be553ea50eeaf58129778d7875ceaca241d935/README.md?download=true&etag=%227486643c201bc6569cf0176d540d01c426410465%22#:~:text=features%3A%20,lichess). We verify that the data is loaded correctly (e.g., checking a few rows or ensuring no critical missing values). Since the dataset is quite large (Lichess provides millions of games[\[2\]](https://huggingface.co/api/resolve-cache/datasets/Lichess/standard-chess-games/a9be553ea50eeaf58129778d7875ceaca241d935/README.md?download=true&etag=%227486643c201bc6569cf0176d540d01c426410465%22#:~:text=Rated%20Standard%20Chess%20Games%20Dataset,%E2%94%9C%E2%94%80%E2%94%80%20month%3D01%20%E2%94%82%C2%A0%C2%A0%20%E2%94%82%C2%A0%C2%A0%20%E2%94%9C%E2%94%80%E2%94%80)), we may limit the data to a manageable subset (for example, games from a certain period or a random sample) for training within a reasonable time.

**Selecting Moves up to _n_:** The key step is preparing the input features that represent the state of the game after _n_ moves. We define _n_ (a configurable parameter at the top of the notebook) as the number of half-moves (plies) from the start of the game to consider. For instance, if _n=10_, that means we look at the position after 10 individual moves (which could be 5 moves by White and 5 by Black). We filter out games that have fewer than _n_ moves, since we can't get a state at move _n_ from those. Each remaining game will provide one training example: the input being the game state after the _n_\-th move, and the output being the final result of the game.

**Parsing Moves to Board State:** To convert the algebraic move list into a usable game state, we utilize the python-chess library. We implemented a helper function (e.g., get_board_after_n_moves(moves_str, n)) which takes the movetext string (the AN column) and simulates the game up to the _n_\-th move. Using python-chess, we initialize a chess.Board() with the standard starting position, then parse and apply moves in sequence. After _n_ moves have been played on the board, we obtain the resulting board state object. This gives us the exact configuration of pieces on the chessboard at that moment. The library's PGN parsing capabilities ensure that standard algebraic notation (including disambiguation and promotion notation) is correctly handled, even including clock or annotation symbols if present (the dataset's movetext may include time stamps %clk which we ignore or strip out[\[3\]](https://huggingface.co/api/resolve-cache/datasets/Lichess/standard-chess-games/a9be553ea50eeaf58129778d7875ceaca241d935/README.md?download=true&etag=%227486643c201bc6569cf0176d540d01c426410465%22#:~:text=games%20include%20Stockfish%20analysis%20evaluations%3A,UCI%20format%5D%28https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FUniversal_Chess_Interface)).

**Feature Engineering - Board Encoding:** The raw board state (from python-chess) must be encoded numerically to feed into the neural network. We use a structured encoding that represents the presence of each type of piece on each square. One effective representation is to create a 8×8×6 tensor representing the board: an 8×8 grid for the board, with 6 layers (one for each piece type: pawn, knight, bishop, rook, queen, king). In each layer, we mark the squares occupied by that piece type - for example, layer 0 for pawns, layer 1 for knights, etc[\[4\]](http://cs230.stanford.edu/projects_spring_2021/reports/66.pdf#:~:text=diverge,a%20white%20piece%20was%20present). We also encode the piece color; one simple approach is to use positive 1 for a white piece and -1 for a black piece on the corresponding layer[\[5\]](http://cs230.stanford.edu/projects_spring_2021/reports/66.pdf#:~:text=rooks%2C%20layer%202%20represented%20knights%2C,A%20single%20game%20was%20then). All empty squares are 0. For instance, if there is a white knight on square b1, in the "knight" layer we put a value 1 at that square's position; a black knight on that square would be -1 instead. After populating all layers, this 8×8×6 tensor can be flattened into a feature vector (of length 8_8_6 = 384) to serve as input to the ANN. This encoding preserves spatial information and piece identity in a way a dense network can process. _(Alternative encoding:_ Another common encoding is 12×8×8 with separate layers for white and black pieces of each type, effectively one-hot encoding each piece-square combination[\[4\]](http://cs230.stanford.edu/projects_spring_2021/reports/66.pdf#:~:text=diverge,a%20white%20piece%20was%20present). This would yield 768 input features. The notebook notes this option, but for simplicity we proceed with the 6-layer signed encoding which gives similar information in a more compact form.)\*

**Preparing Labels:** The output label for each example is the game's final result. We map the result string to a numeric class: e.g., White win = 2, Black win = 0, and Draw = 1 (or any scheme, as long as it's consistent). If predicting draws is enabled (the preferred approach), we have **three classes**. Thus, the labels are one of {"Black win", "Draw", "White win"}. If we were to simplify to binary outcome (only win/lose from White's perspective) due to complexity, then draws could be excluded or treated as half-wins, but the aim is to handle the three-class scenario. The dataset's Result field provides this information directly, as "1-0" for white victory, "0-1" for black victory, and "1/2-1/2" for draw. We convert these into our class indices accordingly.

**Train/Test Split:** After generating the feature vectors and labels for all games in the dataset (or the sampled subset), we split the data into a training set and a test set. A common split is 80% of the examples for training and 20% for testing (we can also carve out a portion of training as a validation set, say 10%, to tune hyperparameters). This split is done randomly to ensure a mix of different game types in each set. We ensure the split is done _after_ shuffling the data, so that any ordering in the original file (which might be chronological) doesn't bias the division. The resulting data structures are typically NumPy arrays or tensors: X_train, y_train, X_test, y_test.

At the end of the preprocessing stage, we have our inputs and outputs ready: each input is a fixed-length numeric vector representing a chess position after _n_ moves, and each output is one of the outcome classes. We proceed to define the neural network that will learn the mapping from input positions to game outcomes.

## Model Design and Architecture

**Neural Network Architecture:** We design a feed-forward artificial neural network (ANN) to perform classification of the game outcome. The architecture is implemented using a deep learning framework (such as TensorFlow Keras for its simplicity in notebooks). The model is a sequential stack of layers:

- **Input Layer:** The input dimension equals the length of the feature vector representing the board. For example, with the chosen encoding, this is 384 features (or 768 if using the 12×8×8 one-hot encoding). We do not explicitly add an "input layer" in Keras, but we specify this input shape for the first hidden layer.
- **Hidden Layers:** We include multiple dense (fully-connected) hidden layers to allow the network to learn complex patterns from the board configuration. For instance, the first hidden layer might have 128 neurons, and the second hidden layer 64 neurons (these sizes can be tuned). Each hidden layer uses a non-linear activation function, typically ReLU (Rectified Linear Unit), which helps the network model non-linear relationships between board features and outcomes. We may also incorporate regularization techniques such as **dropout** (e.g., dropping 20% of neurons between layers) to prevent overfitting, given the high dimensionality of the input.
- **Output Layer:** The output layer provides the prediction for the game result. In the three-class scenario, the output layer has 3 neurons. We apply a _softmax_ activation on this layer to produce a probability distribution over the three outcome classes. That is, the network outputs something like \[P_black_win, P_draw, P_white_win\] which sum to 1. The predicted class is the one with the highest probability. (If we were doing a binary classification, this layer would have 1 neuron with a sigmoid activation or 2 neurons with softmax, but since we aim to distinguish draws as well, we use 3 outputs.)

The rationale for this architecture is to let the network learn an evaluation function for chess positions. Conceptually, this is similar to the **value network** in advanced chess AIs like AlphaZero/Leela, which given a board state evaluates the chances of win, draw, or loss[\[6\]](https://www.lesswrong.com/posts/hPGw7hWYbYyvDcqYK/evidence-against-learned-search-in-a-chess-playing-neural#:~:text=The%20Leela%20network%20is%20not,of%20win%2C%20draw%2C%20loss%20probabilities). Our network is much simpler and is trained supervised on real game outcomes (rather than via self-play reinforcement learning), but the end goal is alike: it tries to predict the result from a position. Fully-connected layers treat the input as a flat vector, which is acceptable here because the encoding includes spatial relationships implicitly (each input neuron corresponds to a specific square and piece type). More sophisticated approaches could use convolutional layers on an 8×8 grid, but to keep the project scope manageable we stick to an ANN (dense network) as initially planned.

**Compilation:** After defining the model layers in the notebook (using Keras Sequential API, for example), we compile the model by specifying the loss function and optimizer. For a multi-class classification, we use **categorical crossentropy** as the loss (or sparse categorical crossentropy if we don't one-hot encode the labels). This loss will measure the error between the predicted probabilities and the true outcome (which can be represented as a one-hot vector: e.g., \[0,0,1\] for White win). The optimizer could be Adam (a popular adaptive learning rate optimizer) to train the network efficiently. We also specify metrics such as **accuracy** so that we can track the fraction of correct predictions during training.

In summary, the model architecture might be: _Input(384) -> Dense(128, ReLU) -> Dense(64, ReLU) -> Dense(3, Softmax)_. This design is kept relatively simple given the data and the fact that we are using a straightforward ANN. The notebook code encapsulates this in a function build_model() that returns a compiled model, making it easy to adjust layer sizes or other parameters in one place.

## Training Process

With data prepared and the model ready, the next notebook section focuses on training the neural network. This is where the model learns from the examples of past games.

**Feeding Data:** We feed the training feature vectors (X_train) and labels (y_train) into the model's training routine. In Keras, this is done via model.fit(), where we specify the number of epochs and batch size. For instance, we might train for 10 epochs initially, with a batch size of 32 (meaning the model sees 32 game positions at a time before updating weights). The training process will iterate through the entire training set multiple times (epochs), each time adjusting the network's weights slightly to reduce the loss.

**Monitoring Training:** As training progresses, we monitor the loss and accuracy on both the training set and validation set (if a validation split is provided in model.fit or we manually evaluate on a validation subset). The notebook prints out the metrics at each epoch. We also store the history of training metrics so that we can plot them later. For example, after training, we produce a line chart showing training vs. validation accuracy over epochs, and similarly for loss. This helps in diagnosing underfitting or overfitting. Ideally, we want to see both training and validation accuracy improving and converging to a high value, without a huge gap between them.

**Handling Class Imbalance:** It's worth noting that in chess data, draws occur less frequently than decisive games (wins/losses), and White wins slightly more often than Black wins (due to first-move advantage)[\[7\]](https://medium.com/@mahmed31098/statistical-analysis-of-the-lichess-dataset-chess-game-using-python-f63eeaf7b592#:~:text=Figure%203). The training process might implicitly handle this if the data is large, but we remain cautious of class imbalance. We can address it by ensuring the loss treats each class equally (since we use categorical crossentropy, each example contributes equally) and by monitoring per-class accuracy. If needed, techniques like class weighting could be applied to give more importance to rarer outcomes (draws) during training. This detail is documented so that if the model has trouble predicting draws, we know how to adjust it.

**Iterative Refinement:** Training an ANN might not get perfect results in one go. The notebook is set up so that one can experiment with different _n_ values, network depths, or training epochs. For example, training on positions after only _n=1_ move may not give a strong signal (since after one pair of moves, most games are far from decided), whereas _n=20_ moves might allow the model to pick up more decisive advantages. We likely try a moderate _n_ (like 10 or 20 moves) first. If the accuracy is low, we might increase _n_ (giving the model more information) or increase model complexity. These experiments are supported by the structure of the notebook - one can re-run the preprocessing with a new _n_ and retrain, or adjust the model structure and observe the outcome.

**Saving the Model:** After training, if the model's performance is satisfactory, we save the model to models/best_model.h5. This uses Keras's model.save() functionality. Saving the model is not strictly required (especially since the notebook retains it in memory for evaluation), but it is useful for later reuse. For example, the UI could load this saved model to make predictions without retraining each time. It also serves as a checkpoint in case we want to compare different trained models.

The training section of the notebook is thus responsible for teaching the neural network. Once training is complete, we move on to evaluating how well the model learned by testing it on unseen game data.

## Evaluation and Testing

After training the model, the notebook proceeds to evaluate its performance on the **test set** that was held out. This gives an unbiased estimate of how well the model can generalize to new games it has never seen.

**Accuracy and Loss:** We first compute the overall accuracy on the test set using the model's evaluate function or by making predictions and comparing to true labels. The accuracy is simply the percentage of games where the predicted outcome matches the actual outcome. We also check the test loss (which should be close to the training loss if the model generalizes well). If the model was trained to predict three classes, the accuracy reported considers a prediction correct only if the exact class (win/draw/loss) was correctly identified. In case we opted for a binary outcome model (White win or not), then accuracy would reflect that simpler criterion. However, our aim is the multi-class outcome, and thus the accuracy accounts for draws as a separate category.

**Detailed Metrics:** To get deeper insight, the notebook may calculate a **confusion matrix** - a 3x3 table for the three classes - showing how often each actual outcome was predicted as each class. This reveals, for example, if the model has a bias (e.g., predicting "White win" too often or struggling to predict "Draw"). We can also derive precision and recall for each class from this matrix. For instance, precision for "White win" tells us what proportion of games the model predicted as White win were truly White wins, and recall tells us what proportion of actual White wins were correctly identified. These metrics can be printed in a small report for completeness.

**Results Interpretation:** Suppose the model achieves an overall accuracy of around, say, 60%. This would indicate some predictive power, given that random guessing among three outcomes would be ~33%. We analyze which outcomes are easiest or hardest: it might turn out that the model is good at identifying one side's overwhelming advantage but poor at recognizing drawn positions. If we see that draws are often misclassified as losses for one side, it might indicate that the model finds it difficult to distinguish a balanced position from one where a player is slightly behind (a common challenge, even for humans, to predict a draw vs a win in many cases). The architecture document notes such possibilities so that we can consider modifications (like including more features, e.g., material advantage, or using a deeper network) if needed in future iterations.

**Sample Test Cases:** As part of testing, we pick a handful of individual games from the test set to examine closely. For each chosen game, we look at the board position after _n_ moves and see what the model predicted versus what actually happened. This is a crucial step not just for evaluation, but also for visualization in the UI, which we describe next.

## User Interface & Visualization

A major goal of the project is to present the model's performance in a **graphical, user-friendly way**. This is handled in the final part of the notebook (and optionally in a separate UI HTML page). There are two main aspects to the visualization: **training performance plots** and **in-game position outcome predictions**.

- **Training Performance Visualization:** After model training, we plot the training and validation accuracy and loss over epochs. This was saved as visuals/training_curves.png during training. In the notebook, this can be directly displayed using Matplotlib. The architecture ensures this plot is generated by capturing the history from Keras training. The chart helps users see if the model is learning steadily, converging, or overfitting (e.g., a divergence between training and validation accuracy curves). By including this in the UI (either embedded in the notebook or via the HTML page), the user can quickly grasp how well the training went and the final accuracy achieved. Key numbers (final accuracy, etc.) are also shown next to the plot for clarity.
- **Example Prediction Visualization:** We enhance the evaluation by showing actual chess boards for some test positions and the model's predictions. Using the **python-chess** library's SVG rendering functionality, we can turn a board state into an image[\[8\]](https://python-chess.readthedocs.io/en/latest/svg.html#:~:text=Renders%20a%20board%20with%20pieces,squares%20as%20an%20SVG%20image). For each example game:
- We retrieve the moves and outcome from the test set.
- We reconstruct the board after _n_ moves (just as done in preprocessing) for that game.
- We use chess.svg.board(board) to get an SVG image of that board with pieces positioned accordingly[\[9\]](https://python-chess.readthedocs.io/en/latest/svg.html#:~:text=SVG%20rendering%EF%83%81). This image is either displayed inline in the Jupyter notebook or saved to visuals/example_position.svg for the UI page.
- We ask the model to predict the outcome given this board's feature vector. The model returns probabilities for each class. We take the argmax as the predicted outcome. We also note the true outcome from the dataset.
- On the visualization, we annotate the board image with text (or a caption) stating "Predicted: White win (60% confidence) - Actual: White win" for example. If the prediction was wrong, it would say e.g. "Predicted: Draw - Actual: Black win" in a highlighted manner.

This way, a reader can _see_ the chess position and intuitively judge if the prediction makes sense. For instance, if the board shows White up a queen and attacking, one would expect the model to predict a White win - if it does, that reinforces trust that the model has learned some chess insight. Conversely, if a position looks roughly equal but the model predicts a decisive result, it might indicate the model is overconfident or keying off factors like rating differences (if those were included as features).

The architecture supports generating multiple such examples. We might use a loop to create visuals for say 5 random test games. These can then be displayed in a grid on the UI page or one by one in the notebook. The combination of the board image and prediction text provides a **graphic testing output** as requested: it directly shows the outcome prediction in context of the actual chess position and result.

- **Local HTML Interface (Optional):** While the Jupyter notebook itself shows these visuals inline, we also considered a simple HTML page (ui/index.html) that could serve as a dashboard. This page could be as basic as displaying the training curve image and a few example board images with their captions. It might use a JavaScript library (like _chessboard.js_ or Lichess's embed widget) to render the chess positions interactively given the FEN strings, but to keep things simple we opted for static images generated by Python (SVG or PNG) which are then embedded. The page does not require a server-side component; it can be opened in a browser after the notebook exports the required images and perhaps writes some HTML. This approach avoids the need for a complex web framework and stays within the project's scope. The architecture document notes this as an optional enhancement - if time permits, a scripted export of results to an HTML file can be implemented. Otherwise, the primary UI is the notebook.

All visualization features are integrated towards the end of the notebook workflow. We ensure that at the end of running the notebook, the user (or developer) is presented with an informative summary: they see how the model trained (graphs), how well it performs overall (accuracy numbers), and concrete examples of its predictions on actual chess positions (board images with predicted vs actual outcomes). This makes the results interpretable and easier to communicate.

## Workflow Summary and Connections

To tie everything together, here's a brief summary of how data and functions flow through the project (as implemented in the Jupyter notebook), highlighting the connections between different parts:

- **Initialization:** The notebook begins by setting configuration values (like N_MOVES = 10 to define _n_, and library imports for pandas, numpy, chess, tensorflow, etc.).
- **Data Loading:** The pandas.read_csv function loads the dataset from data/lichess_games.csv into memory.
- **Preprocessing Functions:** We define functions like parse_movetext_to_board(movetext, n) which uses chess.Board() and applies moves. Similarly, an encode_board(board) function encapsulates the logic of converting a board to our 384-length feature vector. These functions are then used in a loop (or a pandas .apply on the DataFrame) to create the training examples. For each game in the DataFrame, we obtain feature_vector = encode_board(parse_movetext_to_board(game\["AN"\], N_MOVES)) and label = result_to_label(game\["Result"\]). This logic is vectorized as much as possible for efficiency, but conceptually it's one game -> one (features, label) pair.
- **Dataset Split:** We invoke a utility (such as sklearn.model_selection.train_test_split or manual slicing) to shuffle and split the arrays into train and test sets. Now we have X_train, X_test, y_train, y_test.
- **Model Construction:** We call our build_model() function to initialize the Keras model. Internally, this function creates the Sequential model with the specified layers and returns it compiled. The architecture.md (this document) corresponds to that function's design.
- **Training:** Using model.fit(X_train, y_train, epochs=..., batch_size=..., validation_split=0.1), the model is trained. We store the history object. The notebook prints progress each epoch. After training, we optionally call model.save("models/best_model.h5").
- **Plotting:** We use matplotlib to plot history.history\['accuracy'\] and history.history\['val_accuracy'\] (and similarly for loss) over epochs. The plot is saved to visuals/training_curves.png and also displayed inline.
- **Evaluation:** We call model.evaluate(X_test, y_test) to get final test loss and accuracy, printing those results. We then compute predictions on X_test via model.predict(X_test) to generate the confusion matrix and classification report. A small block of code formats and prints these metrics.
- **Visualization of Predictions:** We randomly pick a few indices from the test set. For each, we retrieve the original movetext and result from the DataFrame (we saved the test indices or split in a way to trace back to original data). We reconstruct the board for that test game (using the same parse_movetext_to_board function). Then we call chess.svg.board(board) to get SVG data for the board[\[8\]](https://python-chess.readthedocs.io/en/latest/svg.html#:~:text=Renders%20a%20board%20with%20pieces,squares%20as%20an%20SVG%20image). In the notebook, we use IPython display to show this SVG directly. We also determine the model's predicted class (e.g., by np.argmax(model.predict(feature_vector))) and map it back to a label like "White wins". We know the true label ("Black wins" etc.) from the data. We format a caption below the board image: e.g., **Predicted: Draw; Actual: Draw** or **Predicted: White wins; Actual: Black wins**. This is done for each example. In the UI HTML page (if used), the same images and captions would be laid out nicely, but in the notebook they appear sequentially.
- **UI Page Generation (optional):** If we implement the separate UI page, the notebook would write an HTML file that includes the &lt;img&gt; tags for the charts and board images and some descriptive text or headings. This file could be opened in a browser to present the results interactively. Otherwise, the notebook itself (with all cells run and outputs shown) acts as the final report.

Each part of the process uses the outputs from previous steps (for example, the model is trained on the data prepared in preprocessing; the evaluation uses the trained model; the visualization uses both the model and the test data). Functions defined early (like the parsing and encoding functions) are reused wherever needed (e.g., for generating both training data and later for creating board images in testing). Keeping everything in one notebook allows these functions and variables to be in one scope, making the flow straightforward. Comments in the notebook's code also reference back to these steps (for instance, a comment before training might say "# Build and train the neural network defined above").

Finally, the architecture is documented to be **modular** and extensible. For example, if in the future we want to integrate a more interactive UI (say using a library like Streamlit or Flask), we could refactor the visualization part into a separate script that loads best_model.h5 and serves a web app. Similarly, if we wanted to try a different model (like a convolutional neural network treating the board as an image), we could plug that into the same pipeline - the data preparation would just need to format the input differently (e.g., as an 8×8 matrix instead of a flat vector). The current design serves as a solid foundation: it is clear, single-notebook for simplicity, and covers the full cycle from data to model to user-facing output.

In conclusion, this architecture provides a detailed blueprint of the project. Each file and function has a defined purpose, and the connections between them ensure data flows smoothly from raw game records to a trained model and finally to visual insights. By following this architecture, one can implement the project step-by-step and achieve the goal of predicting chess game outcomes after _n_ moves, with results that are both numerically evaluated and visually interpretable.

**Sources:** The project draws on the Lichess open game database and uses the python-chess library for game state manipulation[\[3\]](https://huggingface.co/api/resolve-cache/datasets/Lichess/standard-chess-games/a9be553ea50eeaf58129778d7875ceaca241d935/README.md?download=true&etag=%227486643c201bc6569cf0176d540d01c426410465%22#:~:text=games%20include%20Stockfish%20analysis%20evaluations%3A,UCI%20format%5D%28https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FUniversal_Chess_Interface)[\[9\]](https://python-chess.readthedocs.io/en/latest/svg.html#:~:text=SVG%20rendering%EF%83%81). The approach of predicting win/draw/loss from a position is conceptually akin to value networks used in chess AIs[\[6\]](https://www.lesswrong.com/posts/hPGw7hWYbYyvDcqYK/evidence-against-learned-search-in-a-chess-playing-neural#:~:text=The%20Leela%20network%20is%20not,of%20win%2C%20draw%2C%20loss%20probabilities). Board state encoding techniques are inspired by prior research on chess position analysis[\[4\]](http://cs230.stanford.edu/projects_spring_2021/reports/66.pdf#:~:text=diverge,a%20white%20piece%20was%20present). These informed our design decisions and ensure the architecture is built on proven practices.

[\[1\]](https://huggingface.co/api/resolve-cache/datasets/Lichess/standard-chess-games/a9be553ea50eeaf58129778d7875ceaca241d935/README.md?download=true&etag=%227486643c201bc6569cf0176d540d01c426410465%22#:~:text=features%3A%20,lichess) [\[2\]](https://huggingface.co/api/resolve-cache/datasets/Lichess/standard-chess-games/a9be553ea50eeaf58129778d7875ceaca241d935/README.md?download=true&etag=%227486643c201bc6569cf0176d540d01c426410465%22#:~:text=Rated%20Standard%20Chess%20Games%20Dataset,%E2%94%9C%E2%94%80%E2%94%80%20month%3D01%20%E2%94%82%C2%A0%C2%A0%20%E2%94%82%C2%A0%C2%A0%20%E2%94%9C%E2%94%80%E2%94%80) [\[3\]](https://huggingface.co/api/resolve-cache/datasets/Lichess/standard-chess-games/a9be553ea50eeaf58129778d7875ceaca241d935/README.md?download=true&etag=%227486643c201bc6569cf0176d540d01c426410465%22#:~:text=games%20include%20Stockfish%20analysis%20evaluations%3A,UCI%20format%5D%28https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FUniversal_Chess_Interface) huggingface.co

<https://huggingface.co/api/resolve-cache/datasets/Lichess/standard-chess-games/a9be553ea50eeaf58129778d7875ceaca241d935/README.md?download=true&etag=%227486643c201bc6569cf0176d540d01c426410465%22>

[\[4\]](http://cs230.stanford.edu/projects_spring_2021/reports/66.pdf#:~:text=diverge,a%20white%20piece%20was%20present) [\[5\]](http://cs230.stanford.edu/projects_spring_2021/reports/66.pdf#:~:text=rooks%2C%20layer%202%20represented%20knights%2C,A%20single%20game%20was%20then) cs230.stanford.edu

<http://cs230.stanford.edu/projects_spring_2021/reports/66.pdf>

[\[6\]](https://www.lesswrong.com/posts/hPGw7hWYbYyvDcqYK/evidence-against-learned-search-in-a-chess-playing-neural#:~:text=The%20Leela%20network%20is%20not,of%20win%2C%20draw%2C%20loss%20probabilities) Evidence against Learned Search in a Chess-Playing Neural Network - LessWrong

<https://www.lesswrong.com/posts/hPGw7hWYbYyvDcqYK/evidence-against-learned-search-in-a-chess-playing-neural>

[\[7\]](https://medium.com/@mahmed31098/statistical-analysis-of-the-lichess-dataset-chess-game-using-python-f63eeaf7b592#:~:text=Figure%203) Statistical analysis of the Lichess dataset (chess game) using Python | by Muhammad Ahmed | Medium

<https://medium.com/@mahmed31098/statistical-analysis-of-the-lichess-dataset-chess-game-using-python-f63eeaf7b592>

[\[8\]](https://python-chess.readthedocs.io/en/latest/svg.html#:~:text=Renders%20a%20board%20with%20pieces,squares%20as%20an%20SVG%20image) [\[9\]](https://python-chess.readthedocs.io/en/latest/svg.html#:~:text=SVG%20rendering%EF%83%81) SVG rendering - python-chess 1.11.2 documentation

<https://python-chess.readthedocs.io/en/latest/svg.html>