# Implementation Plan: Chess Game Outcome Predictor MVP

## Step 1: Environment Setup and Dependencies

Ensure the Jupyter Notebook has all required libraries installed and imported. This includes Python libraries for data handling and chess: - Install/import **pandas** for data manipulation, **numpy** for numerical arrays, **python-chess** for chess board representation and move parsing, and a deep learning framework (e.g. TensorFlow/Keras or PyTorch) for the MLP.  
\- Also import **matplotlib** (or an equivalent library) for plotting training metrics, and **IPython.display** utilities for HTML display if needed.  
\- **Test:** After installation, run import statements for each library to verify they are available. For example, import chess, pandas, tensorflow etc., and print library versions to confirm the environment is set up correctly.

## Step 2: Data Loading

Load the full Lichess game dataset into the notebook. The dataset contains game metadata (e.g. players' Elo ratings, ECO code, opening name) and the moves list: - If the data is in **CSV/Parquet** format, use pandas.read_csv() (or appropriate reader) to load it into a DataFrame. Ensure all columns like _WhiteElo_, _BlackElo_, _Result_, _Moves_ (or _AN_ for algebraic notation moves) are read correctly[\[1\]](https://samiamkhan.github.io/4641Project/#:~:text=Our%20team%20utilized%20a%20data,players%2C%20and%20other%20miscellaneous%20information). For extremely large datasets, consider reading in chunks or using iterators to conserve memory. - If the data is in PGN format, use python-chess PGN parser: iterate through games using chess.pgn.read_game() to extract headers (for Elo, result, etc.) and moves. Accumulate the data into a structured format (list or DataFrame). - **Test:** Confirm the dataset is loaded by checking DataFrame shape (number of games) and inspecting the first row. Verify that expected columns are present (e.g., print df.columns) and sample values (e.g., Result, WhiteElo, Moves) make sense. For example, the _Result_ should be strings like "1-0"/"0-1"/"1/2-1/2", and _Moves_ should contain a sequence of moves in algebraic notation.

## Step 3: Data Exploration and Validation

Before processing further, do basic exploratory checks to validate dataset integrity and inform preprocessing: - **Verify class distribution:** Calculate the frequency of each game outcome (white win, black win, draw) in the dataset. Note if the classes are imbalanced (e.g., typically white wins slightly more often than black[\[2\]](https://en.wikipedia.org/wiki/First-move_advantage_in_chess#:~:text=First,between%2052%20and%2056%20percent)). Ensure all three outcome categories are present as expected. - **Check move length:** Compute the number of plies (half-moves) in each game from the moves list. This helps determine how many games have at least the required _n_ plies. Calculate basic stats (min, median, max plies) to understand the typical game length. - **Test:** Print summary statistics (e.g., outcome counts, average plies per game) to make sure the data looks reasonable. For instance, if historical data shows White wins ~54% of games[\[2\]](https://en.wikipedia.org/wiki/First-move_advantage_in_chess#:~:text=First,between%2052%20and%2056%20percent), verify that the White win proportion in this dataset is in a similar ballpark (allowing some variation based on dataset specifics). Also, confirm that the longest games have a high number of plies, indicating the moves list was read correctly.

## Step 4: Filter Games by Move Length (>= n plies)

Filter out games that do not have at least _n_ plies, since we cannot use them for a prediction based on the first _n_ moves: - Determine the value of _n_ (this can be a parameter set earlier in the notebook). Use the move counts computed in Step 3 to filter the DataFrame (e.g., df_filtered = df\[ df\['move_count'\] >= n \]). - This step ensures every remaining game has a well-defined board state after the first _n_ half-moves. Remove or ignore any games shorter than _n_ plies (these might be very short draws or aborted games). - **Test:** After filtering, recompute the minimum plies in df_filtered to confirm it is at least _n_. Also check how many games were dropped (if any) to ensure the filtering logic is correct (e.g., assert df_filtered\['move_count'\].min() >= n). The number of remaining games should be clearly logged.

## Step 5: Label Encoding of Game Outcomes

Prepare the labels for the multi-class classification. We treat the outcome as three classes from the start (no binary simplification)[\[3\]](https://github.com/oindrillac/Chess-outcome-prediction#:~:text=Problem%20Definition%3A): - Map each result string to a numeric class: for example, "1-0" (White win) → 0, "0-1" (Black win) → 1, "1/2-1/2" (Draw) → 2. This mapping ensures a multi-class label. If the dataset uses other notation (like "White" / "Black" / "Draw"), map those accordingly. - Create a new column (or separate array) for the encoded label. If using Keras, you may later convert this to one-hot encoding or use sparse categorical labels directly. - **Test:** Verify the mapping by printing a few examples (e.g., first 5 labels vs original results) to ensure they are encoded correctly. Also, confirm that the set of unique encoded labels is {0,1,2} (representing the three outcomes). The distribution of these numeric labels should match the original outcome distribution from Step 3.

## Step 6: Board Reconstruction with python-chess

Use the **python-chess** library to reconstruct the board state after the first _n_ plies for each game. This will yield the position that will serve as input feature (along with other features) for the model: - Define a function get_board_after_n(moves_list, n) that takes the moves of a game (e.g., in algebraic notation) and returns a chess.Board object representing the state after _n_ half-moves. The function will:  
1\. Initialize a chess board with the standard starting position: board = chess.Board() which by default is the initial setup.  
2\. Parse and apply the first _n_ moves in the moves list. If moves are in algebraic notation (SAN), use board.push_san(move) for each move[\[4\]](https://python-chess.readthedocs.io/en/latest/#:~:text=%3E%3E%3E%20board.push_san%28,Bc4). If moves are in UCI or another format, use the appropriate parse method (e.g., board.push_uci() or board.push() with chess.Move objects). Stop after applying _n_ moves, leaving the board in the desired state. 3. Return the resulting board object (or its FEN string for easier storage).  
\- Make sure to handle any edge cases: e.g., if a game has exactly _n_ or slightly more moves, only apply _n_. (Games shorter than _n_ were filtered out in Step 4.) - **Test:** Use a small subset of games to verify this function. For example, take a known game or create a test sequence of moves (like "e4", "e5", "Nf3", "Nc6", ... for _n_\=4) and manually verify the resulting board. You can print the board in ASCII (print(board)) or FEN to confirm piece positions match expectations. If possible, cross-check with an external source or the game record for correctness.

## Step 7: Feature Encoding for Board State

Encode the chess board into a flat numeric feature vector suitable for the MLP. We will use a one-hot encoding for pieces on each square, plus include additional metadata features (like player ratings): - Represent the board as a 64-square vector with a one-hot encoding for the piece on each square. One common approach is to use 12 possible piece types (6 for white, 6 for black) and encode each square as a 12-dimensional one-hot vector indicating which piece (if any) occupies it[\[5\]](https://fpga.mit.edu/videos/2022/team36/report.pdf#:~:text=The%20current%20game%20is%20represented,moves%20with%20an%20odd%20length). This yields 64×12 = 768 input features for the board position (squares with no piece can be represented by all zeros, effectively 12 zeros). Alternatively, include an "empty" category making it 13, but using 12 and treating empty as all-zero is typical.  
\- Append additional features from the dataset that may help prediction. For example, include **WhiteElo** and **BlackElo** as two numeric features (or their difference) to provide the rating context[\[6\]](https://samiamkhan.github.io/4641Project/#:~:text=predicted%20using%20only%20the%20knowledge,advantages%20of%20using%20certain%20openings). Player ratings significantly influence outcomes, so incorporating them can improve the model's accuracy[\[6\]](https://samiamkhan.github.io/4641Project/#:~:text=predicted%20using%20only%20the%20knowledge,advantages%20of%20using%20certain%20openings)[\[7\]](https://samiamkhan.github.io/4641Project/#:~:text=take%20precision%20and%20recall%20into,behaved%20very%20similarly%20to%20accuracy). Other possible features: the ECO code or opening name could be one-hot encoded, but to keep the MVP simple, we can omit or handle these later if needed. - Implement a function encode_board(board, white_elo, black_elo) that returns a feature vector. It will iterate over all 64 squares of the board and for each square produce a one-hot vector for the piece (e.g., use board.piece_at(square) to get piece info). Then append the Elo ratings (e.g., normalized or as-is) to the vector. The final feature vector length might be 768 + 2 = 770 (if two Elo features are added). - **Test:** Verify the encoding on a known board. For instance, test encode_board on the initial position: the output should have ones corresponding to the initial piece layout (e.g., one-hot for white rooks at positions 0 and 7, etc.) and zeros elsewhere. Check that the length of the vector is correct (print len(feature_vector)) and that the Elo ratings are correctly placed in the vector. You can also test on an empty board or a board with a single piece to ensure the encoding logic is correct (e.g., only one one-hot active for that piece).

## Step 8: Feature Extraction Pipeline for All Games

Apply the board reconstruction (Step 6) and feature encoding (Step 7) to the entire filtered dataset to generate the training data (X, y): - Iterate over each game in the filtered dataset. For each game, retrieve the moves list and apply get_board_after_n(moves, n) to obtain the board after _n_ plies. Then use encode_board(board, WhiteElo, BlackElo) to get the feature vector for that game. Collect all such vectors into a feature matrix **X**. Collect the corresponding label (from Step 5) into the label array **y**. - Because the full dataset can be very large, consider memory management: if it fits in memory, you can convert X and y into numpy arrays directly. If it's too large, use an incremental approach (e.g., process in batches and perhaps save intermediate results to disk or use a data generator for training). For MVP simplicity, assume we can process in memory, but structure the code in a way that can be refactored to a generator if needed (for example, by yielding features for each game instead of holding all at once). - Monitor the time and memory during this step. It may be the longest step due to parsing and encoding each game position. - **Test:** After processing, check the shape of the feature matrix X (it should be \[number_of_games, feature_vector_length\]) and label array y (length = number_of_games). For sanity check, ensure that the number of samples matches the filtered dataset length. Also, inspect a few sample feature rows: pick an index, look at X\[i\] and y\[i\], and cross-verify with the original game. For example, decode the FEN from the first _n_ moves of that game using python-chess and ensure that the encoded vector's one-hot positions correspond to the pieces in that FEN, and that y\[i\] matches the game's result.

## Step 9: Train-Test Split

Split the dataset into a training set and a test set to evaluate generalization: - Use an 80/20 split (or similar) between training and testing. For example, leverage sklearn.model_selection.train_test_split with test_size=0.2. Use the labels to stratify the split if class imbalance is a concern (this ensures each set has a similar outcome distribution). - Keep the test set aside untouched for final evaluation. The training set can further be split into training and validation if needed (or use Keras's validation_split parameter during training). - **Test:** Verify the split by printing the shapes of the resulting sets (e.g., X_train.shape, X_test.shape). Ensure that no data leakage occurred: the intersection of game IDs (if available) or some identifier between train and test should be empty. Also, confirm the class distribution in train vs test remains similar (e.g., use np.bincount(y_train) vs y_test). There should be no unexpected skew after the split.

## Step 10: MLP Model Design

Design a Multi-Layer Perceptron neural network that takes the encoded features and predicts the game outcome. This network will handle a multi-class classification (three outcomes) from the start[\[3\]](https://github.com/oindrillac/Chess-outcome-prediction#:~:text=Problem%20Definition%3A): - **Input layer:** size equals the length of the feature vector. For example, if using 768 one-hot board features + 2 rating features, input dimension = 770.  
\- **Hidden layers:** one or more Dense layers with non-linear activations. For MVP, start with a simple architecture, e.g., one hidden layer with 128 neurons, or two hidden layers with 128 and 64 neurons. Use **ReLU** activation for hidden layers. (You might experiment with layer sizes; more complex patterns in data could benefit from deeper networks, but watch for overfitting.) - **Output layer:** a Dense layer with 3 neurons (one for each outcome class) and a **softmax** activation to produce a probability distribution over the three outcomes. - Set up the loss function and optimizer. Use **categorical cross-entropy** (or sparse categorical cross-entropy if labels are not one-hot) as the loss, and an optimizer like **Adam** for training. Also track metrics like **accuracy**. - If using Keras, construct the model sequentially or with the functional API, then compile it with the chosen loss and optimizer. Print a model summary to confirm the layer shapes. - **Test:** Check the model definition by inspecting the summary (model.summary() in Keras or equivalent). The summary should show the input shape matching the feature vector length and output shape of (None, 3) for the three classes. Ensure the parameter counts are as expected (e.g., input_layer_weights = input_dim \* hidden_units, etc.). This step is purely to verify the model is built as intended before training. Additionally, you can run a single forward pass with a dummy input (e.g., model.predict(np.zeros((1,770)))) to ensure it produces a 3-element output that sums to 1 (a valid probability distribution).

## Step 11: Model Training

Train the MLP model on the training data, using a portion of it for validation to monitor performance: - Determine a reasonable number of **epochs** (for instance, start with 10 or 20) and a **batch size** (e.g., 32 or 64). Use the training set to fit the model. In Keras, call model.fit(X_train, y_train, epochs=E, batch_size=B, validation_split=0.1, verbose=1). The validation_split will hold out 10% of training data to validate each epoch. This allows tracking validation loss/accuracy to detect overfitting.  
\- Monitor the training process. Ideally, the training accuracy should increase and the loss decrease over epochs, and the validation metrics should also improve and then stabilize. If validation performance starts deteriorating while training accuracy improves, the model might be overfitting - consider using **early stopping** or reducing complexity in that case. - Use multi-class accuracy as a metric. Since this is a multi-class problem, accuracy is the proportion of games where the predicted outcome class matches the actual outcome. - **Test:** After training, inspect the training history. Ensure that by the final epoch, the training accuracy is reasonably high and the validation accuracy is not lagging far behind. For example, you might see training accuracy of say 80% and validation 78% (just as an example). If accuracy is very low (near random ~33%), then the model is not learning; you would revisit feature representation or model parameters. Also verify that the model did run for the specified epochs and that there were no issues like NaNs in loss, etc. This can be done by printing the history.history or plotting the curves (which will be done in the next step).

## Step 12: Model Evaluation on Test Set

Evaluate the trained model on the independent test set that was set aside to measure true generalization: - Use the model to predict outcomes for **X_test** and compare with **y_test**. In Keras, you can call model.evaluate(X_test, y_test) to get the loss and accuracy on the test data. Additionally, get the predicted class labels for test samples (e.g., y_pred = model.predict(X_test) and take argmax to get the class with highest probability for each sample). - Compute the overall test **accuracy** and other metrics. For deeper analysis, compute a **confusion matrix** to see how often each outcome is mistaken for another. This can be done with scikit-learn's confusion_matrix(y_test, y_pred) or manually. The confusion matrix will have dimensions 3x3 for \[WhiteWin, Draw, BlackWin\] and can highlight, for example, if draws are often being misclassified as wins or vice versa. - If desired, calculate precision and recall for each class to understand the model's performance per outcome (though with three classes of game results, accuracy and the confusion matrix might suffice for MVP). - **Test:** Print out the test accuracy and confirm it is in a reasonable range (for instance, it should significantly exceed 33%, which is random guessing for 3 classes). If the team included Elo ratings as features, we expect a decent accuracy because rating plus a few moves can be predictive[\[7\]](https://samiamkhan.github.io/4641Project/#:~:text=take%20precision%20and%20recall%20into,behaved%20very%20similarly%20to%20accuracy). Also, inspect the confusion matrix - for example, ensure that diagonal entries (correct predictions) are the largest in their rows/columns. This step validates that the model is indeed learning some signal about game outcomes. If accuracy is very low, that flags a problem to fix.

## Step 13: Visualize Training Metrics

Create plots to visualize the training process, which will be part of the mini interface for analysis: - Using the history object from training (or data collected during training), plot the **training and validation accuracy** over epochs, and similarly the **training and validation loss** over epochs. A simple line chart with epoch on the x-axis and accuracy on the y-axis (and another for loss) is sufficient. This helps illustrate how the model learned over time and if it overfit. - In code, use matplotlib: e.g., plt.plot(history.history\['accuracy'\], label='train_acc'); plt.plot(history.history\['val_accuracy'\], label='val_acc'), add legend, labels, etc. Do the same for loss. Ensure the plots are clear and labeled. - Optionally, the metrics can be saved to an image file (PNG) for embedding in the HTML report later, or we can directly embed the matplotlib output. - **Test:** Display the plots in the notebook to ensure they show up correctly. The training curves should make sense (e.g., generally increasing accuracy). Check that both training and validation metrics are plotted and that the legend is correct. If the curves show anomalies (like jagged fluctuations or divergence), note them for discussion or tuning. For example, if validation accuracy starts decreasing while training goes up, the plot will clearly show that (indicating overfitting). For the MVP, the main point is that we have a visual of the training dynamics.

## Step 14: Generate Sample Predictions for Visualization

To make the results more interpretable, select a few example games from the test set and visualize their board positions along with the model's prediction vs the actual outcome: - Choose a handful of test samples (for instance, 3 to 5 games) - perhaps some where the model was correct and some where it was wrong, to provide insight. For each selected game:  
\- Take the feature input (which includes the board position after _n_ moves and other features) and get the model's predicted probabilities or class. Determine the predicted outcome class (White win / Draw / Black win) and also note the actual outcome from y_test.  
\- Reconstruct the board from the features so that we can display it. Since we have the moves or the board state, we can obtain a chess.Board for the first _n_ plies of that game (using the functions from Steps 6-7 in reverse: either store the board from when we built features, or call get_board_after_n again for that game's moves).  
\- Use chess.svg.board(...) to create an SVG image of the board position[\[8\]](https://python-chess.readthedocs.io/en/latest/svg.html#:~:text=,0000cccc). For a nicer visualization, you might highlight the last move or use a specific orientation, but a basic board diagram is fine.  
\- Annotate the prediction and actual result. This can be done by either overlaying text on the image or simply using captions. For example, in the HTML interface, we might have a caption like "Predicted: White win, Actual: Draw" below the board image. - Collect these visuals and texts for the interface. Ensure each chosen example is clearly identified with what the model thought and what really happened. - **Test:** For each selected example, display the board SVG in the notebook and print the predicted vs actual outcome to verify correctness. Check that the board corresponds to the described scenario (e.g., if it was supposed to be after 10 plies of a Queen's Gambit, ensure pieces on the board reflect that opening). Also confirm that the predicted label matches what you get from model.predict for that sample. This manual check prevents mismatches between the displayed board and the stated prediction.

## Step 15: Build Minimal HTML Interface for Visualization

Create a simple local HTML output that presents the training metrics and the sample predictions with their board diagrams, so that a user can easily view the results: - Construct an HTML layout (as a string or using an HTML builder) that includes:  
\- A section for **Training Metrics**: embed the accuracy/loss plots (from Step 13) either as base64 images or by referencing a saved image file. A simple way is to save the matplotlib plot as "metrics.png" and use an &lt;img src="metrics.png"&gt; tag in the HTML. Provide a heading or paragraph explaining the chart.  
\- A section for **Sample Game Predictions**: for each example from Step 14, include the SVG image of the board. You can embed SVG directly into HTML (since it's XML) or save each as an .svg/.png file and use &lt;img&gt; tags. Alongside each board image, write a short description like "After n plies - Predicted outcome: X, Actual outcome: Y". This gives context to the viewer.  
\- Use basic styling or simply use headings (&lt;h3&gt; or Markdown if outputting in notebook) to separate these sections. Keep it minimal (no complex JavaScript, just static content). - In the Jupyter Notebook, you can display this HTML by using the IPython.display.HTML class or by writing it to a file and rendering it. For example, display(HTML(html_string)) will show the formatted content in the notebook, or you can save to "report.html". - **Test:** Finally, ensure the HTML interface works. If using display(HTML), the images and SVGs should appear inline with the text. Verify that the training plot is visible and each board image is correctly shown with the right caption. If using a separate file, open it in a browser to confirm. Check that all three outcome categories are represented in the examples (if possible) and that the information is clear. This step is successful if a non-technical user could open the interface and understand how the model performed - e.g., seeing that the model's training converged (from the plot) and how it predicts some example games (from the boards and labels).

Each of the above steps is designed to be modular and testable in isolation. By following this plan in a single Jupyter Notebook, one can develop a working MVP that reads the full Lichess dataset, trains an MLP to predict game outcomes from the first _n_ moves, and provides both quantitative and visual feedback on the model's performance. The careful isolation of tasks (data prep, model building, evaluation, visualization) ensures that at each stage, we can verify correctness before moving to the next, ultimately leading to a reliable and interpretable prototype.

[\[1\]](https://samiamkhan.github.io/4641Project/#:~:text=Our%20team%20utilized%20a%20data,players%2C%20and%20other%20miscellaneous%20information) [\[6\]](https://samiamkhan.github.io/4641Project/#:~:text=predicted%20using%20only%20the%20knowledge,advantages%20of%20using%20certain%20openings) [\[7\]](https://samiamkhan.github.io/4641Project/#:~:text=take%20precision%20and%20recall%20into,behaved%20very%20similarly%20to%20accuracy) Predicting Chess Match Results Based on Opening Moves | 4641Project

<https://samiamkhan.github.io/4641Project/>

[\[2\]](https://en.wikipedia.org/wiki/First-move_advantage_in_chess#:~:text=First,between%2052%20and%2056%20percent) First-move advantage in chess - Wikipedia

<https://en.wikipedia.org/wiki/First-move_advantage_in_chess>

[\[3\]](https://github.com/oindrillac/Chess-outcome-prediction#:~:text=Problem%20Definition%3A) GitHub - oindrillac/Chess-outcome-prediction: Chess outcome prediction using orientation of pieces on board

<https://github.com/oindrillac/Chess-outcome-prediction>

[\[4\]](https://python-chess.readthedocs.io/en/latest/#:~:text=%3E%3E%3E%20board.push_san%28,Bc4) python-chess: a chess library for Python - python-chess 1.11.2 documentation

<https://python-chess.readthedocs.io/en/latest/>

[\[5\]](https://fpga.mit.edu/videos/2022/team36/report.pdf#:~:text=The%20current%20game%20is%20represented,moves%20with%20an%20odd%20length) fpga.mit.edu

<https://fpga.mit.edu/videos/2022/team36/report.pdf>

[\[8\]](https://python-chess.readthedocs.io/en/latest/svg.html#:~:text=,0000cccc) SVG rendering - python-chess 1.11.2 documentation

<https://python-chess.readthedocs.io/en/latest/svg.html>